{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsw9r2yJ+8Pad3gYnKr8Gy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GavishKapoor/ML-Assignment-Regression-Gavish-Kapoor.ipynb/blob/main/ML_Assignement_Regression_Gavish_Kapoor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Simple Linear Regression**\n",
        "\n",
        "### 1. **What is Simple Linear Regression?**\n",
        "\n",
        "It's a method to predict one variable (Y) using another variable (X). It fits a straight line through the data.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Key assumptions of Simple Linear Regression:**\n",
        "\n",
        "* There is a **linear** relationship between X and Y.\n",
        "* The errors (residuals) are **normally distributed**.\n",
        "* Errors have **constant variance** (homoscedasticity).\n",
        "* Observations are **independent**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **What does 'm' represent in Y = mX + c?**\n",
        "\n",
        "**m is the slope**, meaning how much Y changes when X increases by 1 unit.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **What does 'c' represent in Y = mX + c?**\n",
        "\n",
        "**c is the intercept**, the value of Y when X = 0.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **How do we calculate slope (m) in Simple Linear Regression?**\n",
        "\n",
        "Using the formula:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Purpose of the least squares method:**\n",
        "\n",
        "It finds the **best-fitting line** by minimizing the **sum of squared errors** (differences between actual and predicted Y).\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **What is R² (coefficient of determination)?**\n",
        "\n",
        "It shows how well the model explains the data.\n",
        "\n",
        "* **R² = 1** → perfect fit\n",
        "* **R² = 0** → no relationship\n",
        "\n",
        "---\n",
        "\n",
        "##  **Multiple Linear Regression**\n",
        "\n",
        "### 8. **What is Multiple Linear Regression?**\n",
        "\n",
        "It predicts Y using **two or more** input variables (X₁, X₂, X₃...).\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Main difference between Simple and Multiple Linear Regression:**\n",
        "\n",
        "* **Simple**: 1 input variable\n",
        "* **Multiple**: 2 or more input variables\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Key assumptions of Multiple Linear Regression:**\n",
        "\n",
        "Same as simple linear +\n",
        "\n",
        "* No **multicollinearity** between variables\n",
        "* Linearity between each X and Y\n",
        "\n",
        "---\n",
        "\n",
        "### 11. **What is heteroscedasticity?**\n",
        "\n",
        "When the variance of errors **changes** across data points.\n",
        "It makes the model unreliable.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **How to improve model with high multicollinearity?**\n",
        "\n",
        "* **Remove** highly correlated variables\n",
        "* Use **Principal Component Analysis (PCA)**\n",
        "* Use **Ridge/Lasso** regression\n",
        "\n",
        "---\n",
        "\n",
        "### 13. **Transforming categorical variables:**\n",
        "\n",
        "* **Label Encoding**\n",
        "* **One-Hot Encoding**\n",
        "\n",
        "---\n",
        "\n",
        "### 14. **Role of interaction terms:**\n",
        "\n",
        "They check if two variables combined have an extra effect on Y.\n",
        "Example: X₁ \\* X₂\n",
        "\n",
        "---\n",
        "\n",
        "### 15. **Intercept interpretation (Simple vs Multiple):**\n",
        "\n",
        "* **Simple**: Value of Y when X = 0\n",
        "* **Multiple**: Y when **all** Xs = 0 (not always meaningful)\n",
        "\n",
        "---\n",
        "\n",
        "### 16. **Significance of slope in regression:**\n",
        "\n",
        "It shows **how much Y changes** when X increases by 1 unit. It helps in **predicting trends**.\n",
        "\n",
        "---\n",
        "\n",
        "### 17. **How intercept provides context:**\n",
        "\n",
        "The intercept gives the **starting point** of Y when all inputs are zero.\n",
        "\n",
        "---\n",
        "\n",
        "### 18. **Limitations of R² as the only measure:**\n",
        "\n",
        "* It increases with more variables even if they don’t help\n",
        "* Doesn’t tell about **overfitting**\n",
        "  Use **Adjusted R²** or **Cross-validation** instead.\n",
        "\n",
        "---\n",
        "\n",
        "### 19. **Large standard error for a coefficient:**\n",
        "\n",
        "It means the model is **not confident** in the estimate. Coefficient might be **insignificant**.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. **Heteroscedasticity in residual plots:**\n",
        "\n",
        "You’ll see a **fan or cone shape** instead of a flat spread.\n",
        "Important to fix it for better predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 21. **High R² but low Adjusted R²?**\n",
        "\n",
        "It means you added **useless variables** that don’t help the model.\n",
        "\n",
        "---\n",
        "\n",
        "### 22. **Why scale variables in Multiple Regression?**\n",
        "\n",
        "To ensure all variables are on the **same scale**.\n",
        "This helps model learn better and faster, especially for regularized models like Ridge/Lasso.\n",
        "\n",
        "---\n",
        "\n",
        "## **Polynomial Regression**\n",
        "\n",
        "### 23. **What is Polynomial Regression?**\n",
        "\n",
        "It fits a **curved line** instead of a straight one by adding powers of X (like X², X³...).\n",
        "\n",
        "---\n",
        "\n",
        "### 24. **Difference from Linear Regression:**\n",
        "\n",
        "* Linear: Straight line\n",
        "* Polynomial: **Curved line** (can handle complex relationships)\n",
        "\n",
        "---\n",
        "\n",
        "### 25. **When is Polynomial Regression used?**\n",
        "\n",
        "When the data has a **non-linear trend** that a straight line can’t fit well.\n",
        "\n",
        "---\n",
        "\n",
        "### 26. **General equation for Polynomial Regression:**\n",
        "\n",
        "$$\n",
        "Y = a + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 27. **Can it be applied to multiple variables?**\n",
        "\n",
        "Yes, you can apply polynomial terms to multiple variables (like X₁², X₁X₂, etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### 28. **Limitations of Polynomial Regression:**\n",
        "\n",
        "* **Overfitting** if degree is too high\n",
        "* **Hard to interpret**\n",
        "* Sensitive to outliers\n",
        "\n",
        "---\n",
        "\n",
        "### 29. **How to evaluate model fit for polynomial degree?**\n",
        "\n",
        "* Use **R² and Adjusted R²**\n",
        "* Use **Cross-validation**\n",
        "* **Plot the curve** to see fit visually\n",
        "\n",
        "---\n",
        "\n",
        "### 30. **Why is visualization important?**\n",
        "\n",
        "It helps you **see** if the curve fits the data well or is too complex or too simple.\n",
        "\n",
        "---\n",
        "\n",
        "### 31. **How is Polynomial Regression implemented in Python?**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "cC4kI1n_EXlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ph6UgtF2Ei3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}